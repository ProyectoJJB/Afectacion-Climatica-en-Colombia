{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhlcp97wDM9r",
        "outputId": "22c4dcc2-503f-4e91-8ab3-8b0d91913163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sodapy\n",
            "  Downloading sodapy-2.2.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: requests>=2.28.1 in /usr/local/lib/python3.10/dist-packages (from sodapy) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.1->sodapy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.1->sodapy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.1->sodapy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.1->sodapy) (2024.8.30)\n",
            "Downloading sodapy-2.2.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: sodapy\n",
            "Successfully installed sodapy-2.2.0\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title Setup\n",
        "!pip install sodapy\n",
        "!pip install unidecode\n",
        "import pandas as pd\n",
        "import requests\n",
        "from sodapy import Socrata\n",
        "from google.colab import drive\n",
        "import os\n",
        "import time\n",
        "from IPython.display import display, Markdown\n",
        "from google.colab import drive\n",
        "from unidecode import unidecode\n",
        "from requests.exceptions import ReadTimeout\n",
        "\n",
        "client = Socrata(\"www.datos.gov.co\", None)\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u6hMq14FC_XA",
        "outputId": "0f640cb7-7f67-460a-f76e-1f8a88a779de"
      },
      "outputs": [],
      "source": [
        "def download_and_save_data_chunks_departamento(resource_id, filename,departamento,tipo,chunk_size=500000, limit=None):\n",
        "    chunk_number = 1\n",
        "    path = \"/content/drive/My Drive/DANE/chunks/\"+tipo+\"/\"\n",
        "    archive = filename.split('/')[-1][:-4]\n",
        "    while os.path.exists(f'{path}{archive}/{archive}_chunk_{chunk_number}.csv'):\n",
        "        chunk_number += 1\n",
        "    if chunk_number > 1:\n",
        "        print(f'Resuming from chunk {chunk_number} on {archive}')\n",
        "        offset = (chunk_number - 1) * chunk_size\n",
        "    else:\n",
        "        offset = 0\n",
        "    if limit is None:\n",
        "        metadata = client.get_metadata(resource_id)\n",
        "        limit = int(metadata['columns'][0]['cachedContents']['largest'])\n",
        "    while offset < limit:\n",
        "        results = client.get(resource_id,where=f\"departamento='{departamento}'\", limit=chunk_size, offset=offset)\n",
        "        if not results:\n",
        "            print(f'❌ No existen más datos para{archive}')\n",
        "            break\n",
        "        chunk_filename = f'{path}{archive}/{archive}_chunk_{chunk_number}.csv'\n",
        "        pd.DataFrame.from_records(results).to_csv(chunk_filename, index=False)\n",
        "        print(f'✅ Downloaded and saved chunk {chunk_number} to {chunk_filename}')\n",
        "        offset += chunk_size\n",
        "        chunk_number += 1\n",
        "\n",
        "departamentos = [\n",
        "'AMAZONAS',\n",
        "'ANTIOQUIA',\n",
        "'ARAUCA',\n",
        " 'ARCHIPIELAGO DE SAN ANDRES PROVIDENCIA Y SANTA CATALINA',\n",
        " 'ARCHIPIELAGO DE SAN ANDRES, PROVIDENCIA Y SANTA CATALINA',\n",
        "'ARCHIPIÉLAGO DE SAN ANDRES PROVIDENCIA Y SANTA CATALINA',\n",
        "'ATLANTICO',\n",
        " 'ATLÁNTICO',\n",
        " 'BOGOTA',\n",
        " 'BOGOTA D.C.',\n",
        " 'BOGOTÁ',\n",
        " 'BOLIVAR',\n",
        " 'BOLÍVAR',\n",
        "'BOYACÁ',\n",
        " 'CALDAS',\n",
        " 'CAQUETA',\n",
        " 'CAQUETÁ',\n",
        " 'CASANARE',\n",
        " 'CAUCA',\n",
        " 'CESAR',\n",
        " 'CHOCO',\n",
        " 'CHOCÓ',\n",
        " 'CORDOBA',\n",
        "'CUNDINAMARCA',\n",
        " 'CÓRDOBA',\n",
        " 'GUAINÍA',\n",
        " 'GUAVIARE',\n",
        " 'HUILA',\n",
        " 'LA GUAJIRA',\n",
        " 'MAGDALENA',\n",
        " 'META',\n",
        " 'NARINO',\n",
        " 'NARIÑO',\n",
        " 'NORTE DE SANTANDER',\n",
        " 'PUTUMAYO',\n",
        " 'QUINDÍO',\n",
        " 'RISARALDA',\n",
        " 'SAN ANDRÉS PROVIDENCIA',\n",
        " 'SANTANDER',\n",
        " 'SUCRE',\n",
        " 'TOLIMA',\n",
        " 'VALLE DEL CAUCA',\n",
        " 'VICHADA'\n",
        " ]\n",
        "\n",
        "sleeptime = 1\n",
        "chunk = 300000\n",
        "carpeta = 'LluviaPorDepartamento'\n",
        "idArchivo = \"s54a-sgyg\"\n",
        "\n",
        "def descarga_periodica(chunksize, i = 0):\n",
        "  try:\n",
        "    download_and_save_data_chunks_departamento(idArchivo, f\"/content/drive/My Drive/DANE/lluvia{departamentos[i]}.csv\",departamento=departamentos[i],tipo=carpeta,limit=203761118, chunk_size=chunksize)\n",
        "  except ReadTimeout:\n",
        "    i = (i+1)%len(departamentos)\n",
        "    display(Markdown(f'_ReadTimeout, esperando {sleeptime} segundines_'))\n",
        "    print('')\n",
        "    time.sleep(sleeptime)\n",
        "    descarga_periodica(chunksize,i)\n",
        "descarga_periodica(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRFsKG94MDm2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def concatenate_chunks(filename,carpeta):\n",
        "  chunk_number = 1\n",
        "  all_chunks = []\n",
        "  path = f\"/content/drive/My Drive/DANE/chunks/{carpeta}/\"\n",
        "  archive = filename.split('/')[-1][:-4]\n",
        "  print(f'Now Concatenating {archive}')\n",
        "  while os.path.exists(f'{path}{archive}/{archive}_chunk_{chunk_number}.csv'):\n",
        "    print(f'⏳ Concatenating chunk {chunk_number}')\n",
        "    try:\n",
        "      all_chunks.append(pd.read_csv(f'{path}{archive}/{archive}_chunk_{chunk_number}.csv'))\n",
        "      chunk_number += 1\n",
        "    except Exception as e:\n",
        "      print(f\"Error leyendo chunk {chunk_number}: {e}\")\n",
        "      break\n",
        "  if all_chunks:\n",
        "    print(f'Concatenating...')\n",
        "    final_df = pd.concat(all_chunks, ignore_index=True)\n",
        "    final_df.to_csv(filename, index=False)\n",
        "    print(f'💾 Toddos los chunks concatenados en {filename}')\n",
        "    for i in range(1, chunk_number):\n",
        "      try:\n",
        "        os.remove(f'{path}{archive}/{archive}_chunk_{i}.csv')\n",
        "      except Exception as e:\n",
        "        print(f\"Error rermoviendo chunk {i}: {e}\")\n",
        "    print(\"Chunk removido.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykUle4bbMIf6"
      },
      "outputs": [],
      "source": [
        "carpeta = 'LluviaPorDepartamento'\n",
        "\n",
        "for departamento in departamentos:\n",
        "  concatenate_chunks(f\"/content/drive/My Drive/DANE/CSVs listos/vel/vel{departamento}.csv\",carpeta=carpeta)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
